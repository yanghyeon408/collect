# Prompt Caching: LLM 토큰 비용을 10배 절감하는 기술의 원리

> 이 글은 ngrok 블로그의 "Prompt caching: 10x cheaper LLM tokens, but how?"를 바탕으로 Prompt Caching의 기술적 원리를 쉽게 풀어 설명합니다.

---

## 🎯 한 줄 요약

**Prompt Caching은 Attention 메커니즘의 K(Key)와 V(Value) 행렬을 저장해두고 재사용함으로써, 중복 계산을 제거하여 비용과 지연시간을 대폭 줄이는 기술**

---

## 📌 들어가며: 캐시된 토큰이 대체 뭔가요?

OpenAI와 Anthropic 모두 **캐시된 입력 토큰은 일반 입력 토큰보다 10배 저렴**합니다. Anthropic은 긴 프롬프트에서 **지연시간을 최대 85% 줄일 수 있다**고 주장하며, 실제 테스트에서도 이는 사실로 확인됩니다.

```
┌─────────────────────────────────────────────────────────┐
│                의문점                                    │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Q: "캐시된 토큰"이 정확히 뭔가요?                        │
│                                                         │
│  - 응답을 저장했다가 같은 프롬프트면 재사용?              │
│    → ❌ 아님 (같은 프롬프트여도 매번 다른 응답)          │
│                                                         │
│  - 그럼 GPU 클러스터에서 정확히 뭘 저장하는 건가요?      │
│    → 이 글에서 설명할 내용!                             │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

공식 문서들은 prompt caching을 **"어떻게 사용하는지"**는 잘 설명하지만, **"정확히 무엇이 캐시되는지"**는 명확히 말하지 않습니다. 이 글에서는 LLM이 어떻게 동작하는지부터 시작해서, 캐시되는 정확한 데이터가 무엇인지 알아봅니다.

---

## 🧠 LLM 아키텍처 개요

LLM은 본질적으로 **거대한 수학 함수**입니다. 숫자의 시퀀스를 입력받아 숫자를 출력합니다. 내부에는 수십억 개의 연산으로 이루어진 그래프가 있습니다.

```
┌─────────────────────────────────────────────────────────┐
│                  LLM 아키텍처 구조                        │
├─────────────────────────────────────────────────────────┤
│                                                         │
│     ┌──────────────┐                                   │
│     │  Tokenizer   │  ← 텍스트 → 토큰 변환              │
│     └──────┬───────┘                                   │
│            │                                            │
│            ▼                                            │
│     ┌──────────────┐                                   │
│     │  Embedding   │  ← 토큰 → 벡터 변환               │
│     └──────┬───────┘                                   │
│            │                                            │
│            ▼                                            │
│     ┌──────────────────────────────────────┐          │
│     │           Transformer (반복)          │          │
│     │  ┌────────────┐  ┌────────────────┐  │          │
│     │  │ Attention  │→ │  Feedforward   │  │          │
│     │  └────────────┘  └────────────────┘  │          │
│     └──────┬───────────────────────────────┘          │
│            │                                            │
│            ▼                                            │
│     ┌──────────────┐                                   │
│     │    Output    │  ← 다음 토큰 예측                  │
│     └──────────────┘                                   │
│                                                         │
│  ⭐ Prompt Caching이 일어나는 곳: Attention             │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 추론(Inference) 루프

```python
prompt = "What is the meaning of life?"

tokens = tokenizer(prompt)
while True:
    embeddings = embed(tokens)
    for (attention, feedforward) in transformers:
        embeddings = attention(embeddings)
        embeddings = feedforward(embeddings)
    output_token = output(embeddings)
    if output_token == END_TOKEN:
        break
    tokens.append(output_token)  # 출력 토큰을 입력에 추가!

print(decode(tokens))
```

핵심 포인트: **매 반복마다 출력 토큰이 입력에 추가**됩니다. LLM은 좋은 답변을 생성하기 위해 전체 컨텍스트가 필요하기 때문입니다.

---

## 📝 Step 1: Tokenizer - 텍스트를 숫자로

토크나이저는 프롬프트를 작은 청크로 자르고, 각 청크에 정수 ID를 할당합니다.

```
┌─────────────────────────────────────────────────────────┐
│              토큰화 예시: "Check out ngrok.ai"           │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  텍스트:  "Check"  " out"  " ng"  "rok"  ".ai"          │
│              │       │       │      │      │            │
│              ▼       ▼       ▼      ▼      ▼            │
│  토큰:     4383     842    1657  17690  75584           │
│                                                         │
│  → 같은 프롬프트는 항상 같은 토큰 시퀀스로 변환          │
│  → 대소문자 구분됨 ("Will" vs "will" = 다른 토큰)        │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

토큰은 LLM의 입출력 기본 단위입니다. ChatGPT의 응답이 한 글자씩 스트리밍되는 것처럼 보이는 이유는, 실제로 **한 토큰씩 생성되기 때문**입니다.

---

## 📊 Step 2: Embedding - 의미의 공간으로

토큰(정수)을 **n차원 공간의 점(벡터)**으로 변환합니다. 이것이 **임베딩**입니다.

### 왜 임베딩이 필요한가?

```
┌─────────────────────────────────────────────────────────┐
│                 임베딩의 필요성                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  두 문장의 유사성을 판단하려면?                          │
│                                                         │
│  - 슬픔, 재미, 길이, 어조, 언어, 구조...                │
│  - 수많은 "차원"에서 비교 가능                          │
│                                                         │
│  토큰(정수)에는 차원이 없음                              │
│  임베딩(벡터)에는 수천 개의 차원이 있음!                 │
│                                                         │
│  예시:                                                  │
│  - 3차원 임베딩: [10, 4, 2] = x=10, y=4, z=2 위치       │
│  - 실제 모델: 수천~만 개 이상의 차원                    │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 임베딩 룩업 과정

```python
# 학습 중 생성됨, 추론 중에는 변하지 않음
EMBEDDINGS = [...]  # 각 토큰별 벡터

def embed(tokens):
    return [EMBEDDINGS[token] for token in tokens]
```

```
토큰 [75, 305, 284, 887]
         │
         ▼
┌─────────────────────────────────────────┐
│  [[-0.1, 0.5, 0.3],   ← 토큰 75의 임베딩 │
│   [0.2, -0.4, 0.1],   ← 토큰 305의 임베딩│
│   [0.7, 0.2, -0.5],   ← 토큰 284의 임베딩│
│   [-0.3, 0.6, 0.4]]   ← 토큰 887의 임베딩│
└─────────────────────────────────────────┘
         │
         ▼
    임베딩 행렬 (4×3)
```

임베딩은 **토큰의 의미적 표현**입니다. 학습 과정에서 비슷한 의미의 토큰들은 가까운 위치로 이동합니다.

---

## ⚡ Step 3: Attention - 핵심 메커니즘

Attention은 **토큰들 간의 관계를 파악**하고, 각 토큰이 다른 토큰들의 임베딩을 **가중 평균**하여 새로운 임베딩을 만듭니다.

### Attention의 역할

```
┌─────────────────────────────────────────────────────────┐
│              Attention 가중치 예시                       │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  프롬프트: "Mary had a little"                          │
│                                                         │
│  다음 토큰 생성 시 각 토큰의 중요도:                     │
│                                                         │
│     Mary ████████████████████████████████████  63%      │
│     had  ████████████                          16%      │
│     a    ██████████                            12%      │
│    little██████                                 9%      │
│                                                         │
│  → "Mary"가 가장 중요! (Mary had a little "lamb")       │
│  → "Jessica had a little"이었다면 "lamb"이 아닐 것      │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Attention의 수학적 과정

핵심 공식:

```python
# WQ, WK, WV는 학습 중 결정됨 (n×n 행렬)
def attention(embeddings):
    Q = embeddings × WQ    # Query
    K = embeddings × WK    # Key  
    V = embeddings × WV    # Value
    
    scores = Q × transpose(K)
    masked = mask(scores)          # 미래 토큰 마스킹
    weights = softmax(masked)      # 확률 분포로 변환
    
    return weights × V             # 가중 평균
```

### 단계별 시각화

```
┌─────────────────────────────────────────────────────────┐
│             Attention 계산 흐름                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. Q, K 계산 (embeddings × WQ, embeddings × WK)        │
│     ┌─────────┐   ┌─────────┐                          │
│     │embeddings│ × │   WQ    │ = Q (Query)              │
│     │  (4×3)  │   │  (3×3)  │   (4×3)                  │
│     └─────────┘   └─────────┘                          │
│                                                         │
│  2. Scores 계산 (Q × K^T)                               │
│     ┌─────────┐   ┌─────────┐                          │
│     │    Q    │ × │transpose│ = scores                  │
│     │  (4×3)  │   │(K) (3×4)│   (4×4)                  │
│     └─────────┘   └─────────┘                          │
│                                                         │
│  3. Masking (미래 토큰 = -∞)                            │
│     ┌────────────────────┐                             │
│     │ 0.1  -∞   -∞   -∞  │  ← "Mary"만 볼 수 있음      │
│     │ 0.2  0.3  -∞   -∞  │  ← "Mary", "had"           │
│     │ 0.4  0.1  0.2  -∞  │  ← "Mary", "had", "a"      │
│     │ 0.3  0.2  0.1  0.4 │  ← 모든 토큰                │
│     └────────────────────┘                             │
│                                                         │
│  4. Softmax (각 행의 합 = 1)                            │
│     ┌────────────────────┐                             │
│     │1.00 0.00 0.00 0.00 │                             │
│     │0.79 0.21 0.00 0.00 │                             │
│     │0.81 0.13 0.06 0.00 │                             │
│     │0.63 0.16 0.12 0.09 │  ← 최종 가중치              │
│     └────────────────────┘                             │
│                                                         │
│  5. 출력 = weights × V                                  │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 왜 Masking이 필요한가?

"Mary had a little"에서 "had"를 생성할 때, 모델은 "Mary"만 알아야 합니다. "a"나 "little"은 아직 생성되지 않았으니까요. 마스킹은 **미래 토큰이 과거에 영향을 주지 못하게** 합니다.

---

## 💾 드디어! Prompt Caching (KV Caching)

이제 핵심입니다. Attention 애니메이션을 다시 살펴보면:

```
┌─────────────────────────────────────────────────────────┐
│           토큰 추가에 따른 Attention 변화                 │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Step 1: "Mary"                                         │
│  ┌──────┐                                              │
│  │ 1.00 │                                              │
│  └──────┘                                              │
│                                                         │
│  Step 2: "Mary had"                                     │
│  ┌────────────┐                                        │
│  │ 1.00  0.00 │  ← 첫 행 변하지 않음!                  │
│  │ 0.79  0.21 │                                        │
│  └────────────┘                                        │
│                                                         │
│  Step 3: "Mary had a"                                   │
│  ┌──────────────────┐                                  │
│  │ 1.00  0.00  0.00 │  ← 첫 행 변하지 않음!            │
│  │ 0.79  0.21  0.00 │  ← 둘째 행 변하지 않음!          │
│  │ 0.81  0.13  0.06 │                                  │
│  └──────────────────┘                                  │
│                                                         │
│  ⭐ 발견: 이전 행들은 절대 변하지 않는다!                │
│  ⭐ 즉, 매번 전체를 다시 계산할 필요가 없다!             │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 핵심 인사이트

```
"Mary had a little"을 처리한 직후에
"Mary had a"를 다시 계산할 필요가 없다!

→ K와 V 행렬을 캐시하면 중복 계산 제거 가능
```

### KV 캐싱이 적용된 추론

**변경 사항:**
1. 매 반복마다 K와 V 행렬을 캐시
2. 전체 프롬프트 대신 **새 토큰만** 모델에 입력

```
┌─────────────────────────────────────────────────────────┐
│              KV 캐싱 적용 전후 비교                       │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  [캐싱 없이] 5번째 토큰 생성 시:                         │
│                                                         │
│  입력: "Mary had a little" (4개 토큰 전체)              │
│                                                         │
│  embeddings (4×3) × WK (3×3) = K (4×3)    ← 전체 계산  │
│  embeddings (4×3) × WV (3×3) = V (4×3)    ← 전체 계산  │
│                                                         │
│  ────────────────────────────────────────────────────   │
│                                                         │
│  [캐싱 적용] 5번째 토큰 생성 시:                         │
│                                                         │
│  입력: "lamb" (1개 토큰만!)                              │
│                                                         │
│  embedding (1×3) × WK (3×3) = K_new (1×3) ← 1행만 계산 │
│  embedding (1×3) × WV (3×3) = V_new (1×3) ← 1행만 계산 │
│                                                         │
│  K = [cached_K (4×3)] + [K_new (1×3)] = (5×3)          │
│  V = [cached_V (4×3)] + [V_new (1×3)] = (5×3)          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 캐시되는 정확한 데이터

```
┌─────────────────────────────────────────────────────────┐
│              KV Cache의 정체                             │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  캐시되는 데이터:                                        │
│                                                         │
│  K = embeddings × WK                                    │
│  V = embeddings × WV                                    │
│                                                         │
│  ┌─────────────────────────────────────────────────┐   │
│  │                    Cache                         │   │
│  │  ┌─────────────┐    ┌─────────────┐             │   │
│  │  │      K      │    │      V      │             │   │
│  │  │   (5×3)     │    │   (5×3)     │             │   │
│  │  │             │    │             │             │   │
│  │  │ [-0.21,...]│    │ [-0.21,...]│             │   │
│  │  │ [ 0.30,...]│    │ [ 0.30,...]│             │   │
│  │  │ [-0.39,...]│    │ [-0.39,...]│             │   │
│  │  │ [ 0.96,...]│    │ [ 0.96,...]│             │   │
│  │  │ [ 0.34,...]│    │ [-0.49,...]│             │   │
│  │  └─────────────┘    └─────────────┘             │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
│  이것이 "KV 캐싱"이라 불리는 이유!                       │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

**바로 이 K와 V 행렬**이 OpenAI와 Anthropic이 거대한 데이터센터에 저장하여 **10배 저렴한 토큰 가격**과 **훨씬 빠른 응답**을 제공하는 비밀입니다.

---

## 🔄 실제 캐시 동작 방식

### 프로바이더별 캐시 관리

```
┌─────────────────────────────────────────────────────────┐
│              OpenAI vs Anthropic 캐싱 전략               │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  OpenAI:                                                │
│  - 자동 캐싱 (사용자 제어 없음)                          │
│  - 같은 요청 즉시 재전송 시 ~50% 캐시 히트율             │
│  - 긴 컨텍스트에서 성능 변동성 발생 가능                 │
│                                                         │
│  Anthropic:                                             │
│  - 명시적 캐싱 제어 가능                                │
│  - 캐시 요청 시 100% 히트율 (테스트 결과)               │
│  - 캐시 저장 비용 발생                                  │
│  - 예측 가능한 지연시간 필요 시 적합                    │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 부분 캐시 매칭

프롬프트가 **부분적으로만 일치**해도 일치하는 부분의 캐시를 사용할 수 있습니다!

```
┌─────────────────────────────────────────────────────────┐
│                부분 캐시 매칭 예시                        │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  캐시된 프롬프트:                                        │
│  "You are a helpful assistant. Answer in Korean."      │
│                                                         │
│  새 요청:                                               │
│  "You are a helpful assistant. Answer in Korean.       │
│   What is machine learning?"                            │
│                                                         │
│  → 앞부분 일치! 해당 K, V 캐시 재사용                   │
│  → "What is machine learning?" 부분만 새로 계산         │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Temperature와 캐시

```
┌─────────────────────────────────────────────────────────┐
│              Temperature는 캐시에 영향 없음              │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  temperature, top_p, top_k 같은 파라미터는              │
│  추론의 **마지막 단계**에서 토큰 선택에만 영향           │
│                                                         │
│  Attention 메커니즘이 임베딩을 생성한 **이후**에         │
│  적용되므로, 캐시와 무관합니다.                          │
│                                                         │
│  → Temperature를 자유롭게 변경해도                       │
│     캐시 무효화 걱정 없음!                               │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 📈 Prompt Caching의 효과

### 비용 절감

| 프로바이더 | 일반 입력 토큰 | 캐시된 입력 토큰 | 절감률 |
|-----------|--------------|----------------|-------|
| OpenAI | $X | $X/10 | **90%** |
| Anthropic | $X | $X/10 | **90%** |

### 지연시간 감소

```
┌─────────────────────────────────────────────────────────┐
│            Time-to-First-Token 비교                      │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  긴 프롬프트 (수천 토큰) 기준:                           │
│                                                         │
│  캐시 미적용: ████████████████████████████████  100%    │
│  캐시 적용:   ████████                          ~15%    │
│                                                         │
│  → 최대 85% 지연시간 감소 가능!                         │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 🎯 핵심 인사이트 정리

### 1. LLM 추론의 중복성

```
"매 토큰 생성 시 전체 프롬프트를 다시 처리하지만,
 이전 토큰들의 K, V는 절대 변하지 않는다"

→ 이 중복을 제거하면 엄청난 효율 향상
```

### 2. 캐시되는 것의 정체

```
캐시 = K 행렬 + V 행렬

K = embeddings × WK
V = embeddings × WV

응답이 아님! 
중간 계산 결과가 캐시됨!
```

### 3. 왜 응답은 매번 다른가?

```
K, V 캐시 → Attention 가중치 계산
         → 새 임베딩 생성
         → Output layer
         → 확률 분포 (여기서 temperature 적용)
         → 토큰 샘플링 (여기서 무작위성!)

마지막 샘플링 단계에서 무작위성이 적용되므로
같은 캐시를 써도 다른 응답 가능
```

### 4. 실용적 시사점

| 상황 | 권장 전략 |
|------|----------|
| 긴 시스템 프롬프트 반복 사용 | 캐싱 적극 활용 |
| 예측 가능한 지연시간 필요 | Anthropic 명시적 캐싱 |
| 비용 최적화 중요 | 공통 프리픽스 활용 |
| RAG 시스템 | 검색 결과 앞에 캐시 가능 프리픽스 배치 |

---

## 🚀 프로덕션 적용 팁

### 1. 시스템 프롬프트 최적화

```python
# ❌ 비효율적: 매번 다른 순서
prompt1 = f"User question: {q1}\nSystem: You are helpful..."
prompt2 = f"User question: {q2}\nSystem: You are helpful..."

# ✅ 효율적: 공통 프리픽스 유지
prompt1 = f"System: You are helpful...\nUser question: {q1}"
prompt2 = f"System: You are helpful...\nUser question: {q2}"
```

### 2. RAG 시스템에서의 활용

```python
# 검색 결과가 바뀌어도 시스템 프롬프트는 캐시됨
base_prompt = """You are a helpful assistant.
Answer based on the following context:
---
"""

# 각 요청마다
full_prompt = base_prompt + retrieved_context + "\n\nQuestion: " + question
```

### 3. 멀티턴 대화

```python
# 대화 히스토리가 누적되면 자연스럽게 캐시 활용
# 이전 턴의 K, V가 캐시되어 재사용됨
messages = [
    {"role": "system", "content": "..."},      # 캐시됨
    {"role": "user", "content": "첫 질문"},    # 캐시됨
    {"role": "assistant", "content": "답변"}, # 캐시됨
    {"role": "user", "content": "새 질문"},    # 새로 계산
]
```

---

## 📚 참고 자료

- **원문**: [Prompt caching: 10x cheaper LLM tokens, but how?](https://ngrok.com/blog/prompt-caching)
- **Sebastian Raschka**: [Build a Large Language Model (From Scratch)](https://www.oreilly.com/library/view/build-a-large/9781633437166/)
- **Andrej Karpathy**: [Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
- **3blue1brown**: [Attention in Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)
- **Transformer Explainer**: [Interactive Visualization](https://poloclub.github.io/transformer-explainer/)

---

*이 글이 도움이 되셨다면 공유해 주세요! 🙏*
