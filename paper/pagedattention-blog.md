# PagedAttention: LLM 서빙의 메모리 혁명을 이끈 기술

> 이 글은 UC Berkeley에서 발표한 논문 "Efficient Memory Management for Large Language Model Serving with PagedAttention" (SOSP'23)의 핵심 내용을 쉽게 풀어 설명합니다.

---

## 🎯 한 줄 요약

**운영체제의 가상 메모리 기법을 LLM에 적용해서, 같은 GPU로 2~4배 더 많은 요청을 처리할 수 있게 만든 기술**

---

## 📌 들어가며: LLM 서빙은 왜 비싼가?

ChatGPT 같은 LLM 서비스를 운영하려면 NVIDIA A100 같은 고가의 GPU가 필요합니다. 그런데 문제는 이 비싼 GPU조차도 초당 몇 개의 요청밖에 처리하지 못한다는 점입니다.

```
💡 LLaMA-13B 기준, A100 GPU 한 대가 처리할 수 있는 요청:
   → 초당 1개 미만!
```

왜 이렇게 느릴까요? 답은 **메모리**에 있습니다.

---

## 🔍 문제의 핵심: KV 캐시란 무엇인가?

LLM이 텍스트를 생성할 때, 한 글자(토큰)씩 순차적으로 만들어냅니다. 이때 이전에 생성한 모든 토큰의 정보를 기억해야 다음 토큰을 만들 수 있습니다.

### KV 캐시의 동작 원리

```
사용자 입력: "서울의 날씨는"

LLM 생성 과정:
┌─────────────────────────────────────────────────────┐
│  "서울의 날씨는" → [KV 저장] → "오늘"               │
│  "서울의 날씨는 오늘" → [KV 저장] → "맑고"          │
│  "서울의 날씨는 오늘 맑고" → [KV 저장] → "..."      │
└─────────────────────────────────────────────────────┘
```

여기서 **KV 캐시(Key-Value Cache)**란, 각 토큰을 처리할 때 계산된 Key와 Value 텐서를 GPU 메모리에 저장해두는 것입니다. 이렇게 해야 같은 계산을 반복하지 않아도 됩니다.

### KV 캐시가 문제인 이유

```
┌────────────────────────────────────────────────────────┐
│                    KV 캐시의 특징                       │
├────────────────────────────────────────────────────────┤
│  📦 크기가 매우 큼                                      │
│     → LLaMA-13B 기준, 하나의 시퀀스당 최대 1.7GB       │
│                                                        │
│  📈 크기가 동적으로 변함                                │
│     → 출력 길이를 미리 알 수 없음                      │
│     → 짧은 답변? 긴 답변? 실행 전까지 모름             │
└────────────────────────────────────────────────────────┘
```

---

## 😰 기존 시스템의 문제: 60~80%의 메모리 낭비

기존 시스템들은 KV 캐시를 위해 **연속된 메모리 공간**을 미리 확보합니다. 마치 주차장에서 대형 버스 자리를 미리 잡아두는 것과 같습니다.

### 기존 방식의 메모리 낭비 패턴

```
┌─────────────────── GPU 메모리 ───────────────────┐
│                                                   │
│  요청 A를 위해 예약된 공간 (최대 2048 토큰 가정)  │
│  ┌─────────────────────────────────────────────┐ │
│  │████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░│ │
│  │ 실제사용    내부 단편화 (낭비!)              │ │
│  └─────────────────────────────────────────────┘ │
│                                                   │
│  요청 B를 위해 예약된 공간                        │
│  ┌─────────────────────────────────────────────┐ │
│  │██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░│ │
│  │   실제사용       내부 단편화 (낭비!)         │ │
│  └─────────────────────────────────────────────┘ │
│                                                   │
│  ░░░░░░░░ 외부 단편화 (사용 불가능한 자투리) ░░░░│
│                                                   │
└───────────────────────────────────────────────────┘
```

**세 가지 주요 낭비 유형:**

| 낭비 유형 | 설명 | 비유 |
|----------|------|------|
| **예약으로 인한 낭비** | 최대 길이만큼 미리 잡아둠 | 100명 올지 모르니 100인분 예약 |
| **내부 단편화** | 예약 공간 중 실제로 안 쓰는 부분 | 10명만 와서 90인분 버림 |
| **외부 단편화** | 메모리 사이사이 자투리 공간 | 테이블 사이 애매한 공간 |

---

## 💡 PagedAttention: 운영체제에서 답을 찾다

논문 저자들은 이 문제를 보고 컴퓨터 운영체제의 **가상 메모리(Virtual Memory)** 기법을 떠올렸습니다.

### 가상 메모리 기법이란?

1960년대에 개발된 기술로, 프로그램이 연속된 메모리를 사용하는 것처럼 보이지만, 실제 물리 메모리는 조각조각 나뉘어 있을 수 있게 해주는 기술입니다.

```
┌─────────────────────────────────────────────────────────┐
│                    가상 메모리 개념                      │
├─────────────────────────────────────────────────────────┤
│                                                         │
│   프로그램이 보는 메모리        실제 물리 메모리        │
│   (논리적, 연속적)              (비연속적 조각들)       │
│                                                         │
│   ┌─────────────┐              ┌───┐                   │
│   │   페이지 0  │─────────────→│ A │                   │
│   ├─────────────┤              └───┘                   │
│   │   페이지 1  │──┐           ┌───┐                   │
│   ├─────────────┤  └──────────→│ B │                   │
│   │   페이지 2  │──┐           └───┘                   │
│   └─────────────┘  │           ┌───┐                   │
│                    └──────────→│ C │                   │
│                                └───┘                   │
│                                                         │
│   "페이지 테이블"이 논리↔물리 주소를 매핑               │
└─────────────────────────────────────────────────────────┘
```

### PagedAttention의 핵심 아이디어

이 가상 메모리 개념을 KV 캐시에 그대로 적용합니다!

```
┌─────────────────────────────────────────────────────────┐
│              PagedAttention 개념 매핑                    │
├─────────────────────────────────────────────────────────┤
│                                                         │
│      운영체제 개념        →      PagedAttention         │
│      ─────────────             ─────────────            │
│      프로세스            →      요청 (Request)          │
│      바이트              →      토큰 (Token)            │
│      페이지              →      KV 블록 (Block)         │
│      페이지 테이블       →      블록 테이블              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 🔧 PagedAttention 동작 방식

### Step 1: KV 캐시를 고정 크기 블록으로 분할

```
기존 방식: 연속된 하나의 큰 덩어리
┌────────────────────────────────────────────────────┐
│           하나의 긴 KV 캐시 (연속 메모리 필요)      │
└────────────────────────────────────────────────────┘

PagedAttention: 고정 크기 블록들로 분할
┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐
│ Block0 │ │ Block1 │ │ Block2 │ │ Block3 │
│(16토큰)│ │(16토큰)│ │(16토큰)│ │(16토큰)│
└────────┘ └────────┘ └────────┘ └────────┘
```

### Step 2: 블록 테이블로 매핑 관리

```
요청: "서울의 날씨는 오늘 맑고 화창합니다"

블록 테이블 (Block Table):
┌──────────────┬──────────────────┐
│ 논리 블록 #  │ 물리 블록 주소    │
├──────────────┼──────────────────┤
│      0       │     GPU 주소 7   │
│      1       │     GPU 주소 2   │  ← 연속일 필요 없음!
│      2       │     GPU 주소 15  │
└──────────────┴──────────────────┘
```

### Step 3: 필요할 때만 블록 할당 (On-demand Allocation)

```
시간 흐름에 따른 동적 할당:

t=0: 프롬프트 처리
     ┌────────┐ ┌────────┐
     │ Block0 │ │ Block1 │
     │ 할당됨 │ │ 할당됨 │
     └────────┘ └────────┘

t=1: 첫 번째 토큰 생성 → Block1에 여유 있음, 추가 할당 불필요

t=5: Block1 가득 참 → 새 블록 필요!
     ┌────────┐ ┌────────┐ ┌────────┐
     │ Block0 │ │ Block1 │ │ Block2 │
     │  가득  │ │  가득  │ │ 새할당 │
     └────────┘ └────────┘ └────────┘
```

---

## 📊 메모리 낭비는 어디로?

### 기존 vs PagedAttention 비교

```
┌──────────────────────────────────────────────────────────┐
│                    메모리 활용 비교                       │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  기존 시스템:                                            │
│  ┌─────────────────────────────────────────────────────┐│
│  │████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░││
│  │  실제  │              낭비 (60-80%)                 ││
│  └─────────────────────────────────────────────────────┘│
│                                                          │
│  PagedAttention:                                         │
│  ┌────┐┌────┐┌────┐┌────┐┌────┐┌──░░┐                  │
│  │████││████││████││████││████││██░░│                  │
│  └────┘└────┘└────┘└────┘└────┘└────┘                  │
│   100%  100%  100%  100%  100%  마지막                   │
│                                 블록만                   │
│                                 일부낭비                 │
│                                                          │
│  → 낭비: 4% 미만! (마지막 블록의 일부만)                 │
└──────────────────────────────────────────────────────────┘
```

---

## 🔄 보너스 기능: 메모리 공유 (Memory Sharing)

PagedAttention의 또 다른 강점은 여러 요청이 **같은 블록을 공유**할 수 있다는 점입니다.

### 사례 1: Parallel Sampling (여러 답변 동시 생성)

같은 프롬프트로 여러 개의 다른 답변을 생성할 때:

```
프롬프트: "인공지능의 미래는"

┌─────────────────────────────────────────────────────────┐
│                                                         │
│  프롬프트 KV 캐시 (공유됨)                               │
│  ┌────────┐┌────────┐                                  │
│  │ Block0 ││ Block1 │ ← 이 블록들을 3개 출력이 공유!    │
│  └───┬────┘└───┬────┘                                  │
│      │         │                                        │
│      ▼         ▼                                        │
│  ┌───────────────────────────────────────┐             │
│  │       블록 테이블 (reference count: 3)│             │
│  └───────────────────────────────────────┘             │
│           │         │         │                         │
│           ▼         ▼         ▼                         │
│      [출력 A]   [출력 B]   [출력 C]                     │
│      "밝다"    "어둡다"   "불확실"                      │
│                                                         │
└─────────────────────────────────────────────────────────┘

기존: 프롬프트 KV 캐시 3벌 저장 (3배 메모리)
PagedAttention: 프롬프트 KV 캐시 1벌만 저장 (1배 메모리)
→ 메모리 55%까지 절약!
```

### 사례 2: Copy-on-Write (CoW)

출력이 분기될 때만 복사합니다:

```
┌─────────────────────────────────────────────────────────┐
│                Copy-on-Write 동작                        │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  초기 상태: 두 출력이 같은 블록 공유                     │
│                                                         │
│  Block 5 ──────────────────┐                           │
│  [ref_count = 2]           │                           │
│  ┌────────────────┐        │                           │
│  │ "인공지능의"   │        │                           │
│  └────────────────┘        │                           │
│         ↑                  ↑                           │
│      출력 A              출력 B                        │
│                                                         │
│  ─────────────────────────────────────────────────────  │
│                                                         │
│  출력 A가 새 토큰 쓸 때:                                │
│                                                         │
│  Block 5                  Block 9 (새로 할당)          │
│  [ref_count = 1]          [ref_count = 1]              │
│  ┌────────────────┐       ┌────────────────┐           │
│  │ "인공지능의"   │       │ "인공지능의"   │ 복사!     │
│  │ "미래는"       │       │ "발전은"       │           │
│  └────────────────┘       └────────────────┘           │
│         ↑                        ↑                     │
│      출력 A                    출력 B                  │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 🏗️ vLLM 시스템 아키텍처

PagedAttention 위에 구축된 vLLM의 전체 구조입니다:

```
┌─────────────────────────────────────────────────────────────┐
│                      vLLM 시스템 구조                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                   Centralized Scheduler              │   │
│  │         (어떤 요청을 언제 처리할지 결정)              │   │
│  └─────────────────────────┬───────────────────────────┘   │
│                            │                                │
│                            ▼                                │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                  KV Cache Manager                    │   │
│  │                                                      │   │
│  │   ┌──────────────┐    ┌──────────────┐              │   │
│  │   │ Block Tables │    │Block Allocator│              │   │
│  │   │ (매핑 정보)  │    │ (블록 할당)   │              │   │
│  │   └──────────────┘    └──────────────┘              │   │
│  │                                                      │   │
│  └─────────────────────────┬───────────────────────────┘   │
│                            │                                │
│           ┌────────────────┼────────────────┐              │
│           ▼                ▼                ▼              │
│    ┌───────────┐    ┌───────────┐    ┌───────────┐        │
│    │  Worker 1 │    │  Worker 2 │    │  Worker 3 │        │
│    │   (GPU)   │    │   (GPU)   │    │   (GPU)   │        │
│    └───────────┘    └───────────┘    └───────────┘        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Preemption (선점) 처리

GPU 메모리가 부족할 때 두 가지 전략:

```
┌─────────────────────────────────────────────────────────┐
│               메모리 부족 시 선점 전략                    │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  전략 1: Swap (스왑)                                    │
│  ┌──────────┐         ┌──────────┐                     │
│  │   GPU    │ ──────→ │   CPU    │                     │
│  │  Memory  │ (일시   │  Memory  │                     │
│  │(블록들) │  이동)  │ (대기)   │                     │
│  └──────────┘         └──────────┘                     │
│                                                         │
│  전략 2: Recompute (재계산)                             │
│  ┌──────────┐                                          │
│  │   GPU    │ ──────→ 블록 삭제                        │
│  │  Memory  │         나중에 다시 계산                  │
│  └──────────┘         (프롬프트+생성토큰 합쳐서)        │
│                                                         │
│  💡 Recompute가 더 빠를 수 있음!                        │
│     이유: 이미 생성된 토큰을 프롬프트처럼 병렬 처리     │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 📈 성능 결과

### Throughput 향상

```
┌─────────────────────────────────────────────────────────┐
│              처리량(Throughput) 비교                     │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  기준: 동일 지연 시간(latency)에서 처리 가능한 요청 수  │
│                                                         │
│  HuggingFace Transformers 대비:                         │
│  ████████████████████████ 최대 24배 향상               │
│                                                         │
│  FasterTransformer 대비:                                │
│  ████████ 2~4배 향상                                   │
│                                                         │
│  Text Generation Inference (TGI) 대비:                  │
│  ███████ 2.2~3.5배 향상                                │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 실제 서비스 검증

LMSYS Chatbot Arena에서의 실제 배포 결과:

| 지표 | 개선 효과 |
|------|----------|
| 일일 평균 요청 처리 | 30,000건 |
| 피크 시간 요청 처리 | 60,000건 |
| 필요 GPU 수 | **50% 감소** |
| 처리량 | 최대 **30배 향상** (HF 대비) |

---

## 🎓 핵심 인사이트 정리

### 1. 시스템 관점의 혁신

```
"LLM 서빙의 병목은 연산이 아니라 메모리다"

연산 최적화에 집중하던 기존 연구들과 달리,
이 논문은 메모리 관리의 비효율성을 정면으로 공략했습니다.
```

### 2. Cross-Domain 아이디어 적용

```
"60년 된 OS 기술이 최신 AI 문제를 해결하다"

1960년대 가상 메모리 → 2023년 LLM 서빙

오래된 아이디어도 새로운 문맥에서 혁신이 될 수 있습니다.
```

### 3. 실용적 임팩트

```
"이론이 아닌 실제 프로덕션에서 검증"

vLLM은 현재 가장 널리 사용되는 LLM 서빙 엔진 중 하나로,
수많은 기업과 연구 기관에서 사용 중입니다.
```

---

## 🔮 후속 연구 및 한계

### 현재 한계점

- 블록 크기가 수동으로 설정됨 (적응형 정책 없음)
- 멀티 GPU에서 KV 캐시 중복 저장 발생
- 디코더 전용 LLM에 최적화 (인코더-디코더 모델은 추가 연구 필요)

### 이후 발전된 기술들

| 기술 | 설명 |
|------|------|
| **Chunked Prefill** | 긴 프롬프트를 청크로 나눠 처리 |
| **Speculative Decoding** | 작은 모델로 먼저 예측, 큰 모델로 검증 |
| **Prefix Caching** | 시스템 프롬프트 캐싱 |
| **FlashAttention 통합** | 커널 레벨 최적화와 결합 |

---

## 🚀 직접 사용해보기

```bash
# 설치
pip install vllm

# 간단한 서버 실행
python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-2-7b-chat-hf
```

```python
# Python에서 사용
from vllm import LLM

llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")
outputs = llm.generate(["서울의 날씨는"])
print(outputs[0].outputs[0].text)
```

---

## 📚 참고 자료

- **논문**: [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
- **공식 GitHub**: [vllm-project/vllm](https://github.com/vllm-project/vllm)
- **vLLM 블로그**: [blog.vllm.ai](https://blog.vllm.ai)

---

*이 글이 도움이 되셨다면 공유해 주세요! 🙏*
