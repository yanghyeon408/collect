# "Attention Is Not What You Need" - AI의 핵심 공식이 뒤집어질 수도?

> 2017년 구글이 "Attention Is All You Need"로 AI 혁명을 일으켰다면,  
> 2025년 이 논문은 "그거 없어도 된다"고 말합니다.

---

## 🤔 잠깐, Attention이 뭔데?

ChatGPT, Claude, Gemini... 요즘 AI들의 핵심 기술이 바로 **Attention(어텐션)**입니다.

쉽게 말하면, **"문장 속 단어들이 서로를 쳐다보는 것"**이에요.

예를 들어 "나는 어제 카페에서 아메리카노를 마셨다"라는 문장에서:

- "마셨다"가 "아메리카노"를 쳐다봄 → "아, 이게 마신 거구나"
- "마셨다"가 "나는"을 쳐다봄 → "아, 이 사람이 마신 거구나"  
- "마셨다"가 "카페에서"를 쳐다봄 → "아, 여기서 마신 거구나"

이렇게 **모든 단어가 다른 모든 단어를 쳐다보면서** 문맥을 파악하는 거죠.

이 기술 덕분에 AI가 글을 이해하고, 번역하고, 대화할 수 있게 된 겁니다.

---

## 😰 근데 뭐가 문제야?

문제는 **계산량**이에요.

단어가 10개면? → 10 × 10 = **100번** 쳐다봐야 함  
단어가 100개면? → 100 × 100 = **10,000번** 쳐다봐야 함  
단어가 1,000개면? → 1,000 × 1,000 = **1,000,000번** 쳐다봐야 함

단어 수의 **"제곱"**만큼 계산이 필요해요. 📈

그래서 긴 문서를 처리하려면 엄청난 컴퓨팅 파워가 필요하고, 이게 비용과 시간 문제로 이어집니다.

---

## 💡 새 논문의 아이디어

이번에 나온 "Attention Is Not What You Need" 논문은 이렇게 말합니다:

> "굳이 모든 단어가 서로 다 쳐다볼 필요 있어?"

학교로 비유하면:

| 기존 Attention | 새로운 방식 |
|---------------|------------|
| 전교생 1,000명이 서로 악수 | 옆자리 친구들이랑만 악수 |
| 1,000 × 1,000 = 백만 번 | 1,000 × 몇 명 = 수천 번 |

**가까운 단어들끼리만 관계를 파악해도 충분하다**는 거예요.

대신 그 관계를 파악하는 방식을 수학적으로 더 똑똑하게 만들었습니다. (Grassmann manifold라는 기하학 개념 사용)

---

## 📊 실험 결과는?

### 테스트 1: 문장 이해력 (SNLI)

"이 두 문장이 같은 말이야, 반대말이야, 상관없어?" 를 맞추는 테스트

| 방식 | 정확도 |
|-----|-------|
| 기존 Attention | 85.11% |
| **새로운 방식** | **85.38%** ✅ |

Attention 없이 **오히려 더 좋은 성능!**

### 테스트 2: 글쓰기 능력 (Wikitext-2)

다음 단어를 예측하는 테스트

| 방식 | 성능 |
|-----|------|
| 기존 Attention | 더 좋음 |
| 새로운 방식 | 10~15% 뒤처짐 ❌ |

아직 글쓰기에서는 Attention을 못 따라잡았어요.

---

## 🎯 그래서 뭐가 중요한 건데?

### ✅ 좋은 점

1. **"Attention이 유일한 답은 아니다"를 증명**
   - 7년간 당연하게 여겨온 공식에 의문을 던짐

2. **계산 효율성**
   - 제곱 → 비례로 계산량 감소 가능성
   - 긴 문서 처리가 더 쉬워질 수 있음

3. **AI 이해하기가 더 쉬워질 수도**
   - 기존 Attention은 너무 복잡해서 "왜 이렇게 판단했는지" 알기 어려움
   - 새 방식은 수학적으로 더 분석하기 쉬운 구조

### ❌ 아직 한계

1. 글쓰기 능력은 뒤처짐
2. 대규모 AI(GPT-4급)에서는 검증 안 됨
3. 실제 속도 최적화 필요

---

## 🔮 앞으로 어떻게 될까?

솔직히 **당장 ChatGPT가 바뀌진 않을 거예요.**

하지만 이 연구가 보여주는 건:

> "지금 방식이 최선이 아닐 수 있다"

2017년에 Attention이 등장해서 AI 세상을 바꿨듯이, 언젠가 이런 새로운 접근법이 다음 혁명을 일으킬 수도 있습니다.

AI에 관심 있다면 기억해둘 만한 논문이에요.

---

## 📝 한 줄 요약

> **ChatGPT 핵심 기술인 Attention, 없어도 된다? 새 논문이 대안을 제시했고, 일부 테스트에서 더 좋은 성능까지 보여줌. AI의 새 장이 열릴 수도!**

---

**📄 논문 링크**: [arxiv.org/abs/2512.19428](https://arxiv.org/abs/2512.19428)

---

*읽어주셔서 감사합니다! 🙏*
