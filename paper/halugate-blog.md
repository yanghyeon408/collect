# HaluGate: 프로덕션 LLM을 위한 실시간 Hallucination 탐지 시스템

> 이 글은 vLLM Semantic Router 팀에서 발표한 "Token-Level Truth: Real-Time Hallucination Detection for Production LLMs" 블로그의 핵심 내용을 정리한 것입니다.

---

## 🎯 한 줄 요약

**LLM이 Tool을 호출해서 정확한 데이터를 받았는데도 틀린 답변을 생성할 때, 밀리초 단위로 이를 감지하고 경고하는 시스템**

---

## 📌 들어가며: Hallucination이 프로덕션 배포를 막는다

LLM을 실제 서비스에 배포하는 데 가장 큰 장애물은 무엇일까요? 바로 **Hallucination(환각)** 입니다.

```
┌─────────────────────────────────────────────────────────┐
│         Hallucination으로 인한 산업별 리스크              │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  ⚖️ 법률    →  존재하지 않는 판례 인용                   │
│  🏥 의료    →  잘못된 약물 상호작용 정보                 │
│  💰 금융    →  허위 재무 데이터 제공                    │
│  🎧 고객지원 →  존재하지 않는 정책 안내                  │
│                                                         │
│  공통점: "그럴듯하게 들리지만 사실이 아닌 정보"          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

문제는 명백한 헛소리가 아닙니다. **정확한 답변 속에 섞여 있는 미묘한 오류**—도메인 전문가나 외부 검증 없이는 잡아내기 어려운 것들이 진짜 위험합니다.

---

## 🔍 문제 상황: Tool은 정확한데 모델이 틀린다?

RAG나 Function Calling을 사용하면 hallucination이 해결될 거라고 생각하기 쉽습니다. 하지만 현실은 다릅니다.

### 실제 발생하는 시나리오

```
┌─────────────────────────────────────────────────────────┐
│                   Extrinsic Hallucination                │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  👤 사용자: "에펠탑은 언제 지어졌나요?"                  │
│                                                         │
│  🔧 Tool 호출: get_landmark_info("Eiffel Tower")        │
│                                                         │
│  📥 Tool 응답 (정확한 데이터):                          │
│  {                                                      │
│    "name": "Eiffel Tower",                             │
│    "built": "1887-1889",        ✅ 정확                 │
│    "height": "330 meters",      ✅ 정확                 │
│    "location": "Paris, France"  ✅ 정확                 │
│  }                                                      │
│                                                         │
│  🤖 LLM 응답:                                           │
│  "에펠탑은 1950년에 지어졌으며, 높이는 500미터입니다."   │
│              ^^^^                  ^^^                  │
│              ❌ 틀림!              ❌ 틀림!              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

이것이 바로 **Extrinsic Hallucination**입니다. 모델에게 Ground Truth가 주어졌는데도 이를 무시하고 틀린 정보를 생성하는 현상입니다.

### 왜 이게 특히 위험한가?

| 요소 | 문제점 |
|------|--------|
| **사용자 신뢰** | Tool이 호출된 걸 보고 "검증된 정보"라고 믿음 |
| **기존 필터 우회** | 유해 콘텐츠가 아니라서 기존 안전장치 통과 |
| **검증 비용** | LLM-as-Judge 방식은 느리고 비쌈 |

---

## 💡 핵심 인사이트: Function Calling = Ground Truth

HaluGate의 핵심 아이디어는 간단합니다.

```
"Function Calling API가 이미 Ground Truth를 제공하고 있다!"

Tool 응답 = RAG의 Retrieved Document와 동일한 역할
          = 검증을 위한 기준점
```

별도의 retrieval 인프라나 GPT-4 Judge가 필요 없습니다. 기존 API 흐름에서 세 가지를 추출하면 됩니다:

```
┌─────────────────────────────────────────────────────────┐
│              HaluGate 검증의 3요소                       │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  ┌─────────────┐                                       │
│  │   Context   │ ← Tool Message 내용 (Ground Truth)    │
│  └─────────────┘                                       │
│         │                                               │
│         ▼                                               │
│  ┌─────────────┐                                       │
│  │  Question   │ ← User Message (의도 파악)            │
│  └─────────────┘                                       │
│         │                                               │
│         ▼                                               │
│  ┌─────────────┐                                       │
│  │   Answer    │ ← Assistant Response (검증 대상)      │
│  └─────────────┘                                       │
│                                                         │
│  핵심 질문: "Answer가 Context에 충실한가?"              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 🚫 왜 LLM-as-Judge가 아닌가?

"GPT-4한테 검증시키면 되지 않나?"라는 생각이 들 수 있습니다. 하지만 프로덕션에서는 문제가 많습니다.

### 성능 비교

| 접근법 | 지연시간 | 비용 | 설명가능성 |
|--------|---------|------|-----------|
| GPT-4 as Judge | 2-5초 | $0.01-0.03/요청 | 낮음 (블랙박스) |
| 로컬 LLM Judge | 500ms-2초 | GPU 연산 | 낮음 |
| **HaluGate** | **76-162ms** | **CPU만 사용** | **높음 (토큰 레벨)** |

### LLM Judge의 구조적 문제

```
┌─────────────────────────────────────────────────────────┐
│              LLM-as-Judge의 알려진 편향들                │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  📍 Position Bias                                       │
│     → 특정 위치의 답변을 선호하는 경향                   │
│                                                         │
│  📏 Verbosity Bias                                      │
│     → 정확도와 무관하게 긴 답변에 높은 점수              │
│                                                         │
│  🪞 Self-Preference                                     │
│     → 자신과 비슷한 스타일의 출력을 선호                 │
│                                                         │
│  🎲 Inconsistency                                       │
│     → 같은 입력에 다른 판단을 내림                       │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

더 빠르고, 저렴하고, 설명 가능한 방법이 필요했습니다.

---

## 🔧 HaluGate: 2단계 탐지 파이프라인

HaluGate는 효율성과 정밀도의 균형을 맞추는 **조건부 2단계 파이프라인**을 구현합니다.

```
┌─────────────────────────────────────────────────────────┐
│               HaluGate 전체 아키텍처                     │
├─────────────────────────────────────────────────────────┤
│                                                         │
│                    사용자 프롬프트                       │
│                          │                              │
│                          ▼                              │
│  ┌─────────────────────────────────────────────────┐   │
│  │         Stage 1: HaluGate Sentinel               │   │
│  │         (프롬프트 분류 - ~12ms)                   │   │
│  │                                                  │   │
│  │    "이 질문이 사실 검증이 필요한가?"              │   │
│  └─────────────────────┬───────────────────────────┘   │
│                        │                                │
│           ┌────────────┴────────────┐                  │
│           │                         │                  │
│           ▼                         ▼                  │
│    ┌──────────────┐         ┌──────────────┐          │
│    │ 창작/코딩/의견│         │   사실 확인   │          │
│    │  → Pass      │         │    필요      │          │
│    └──────────────┘         └──────┬───────┘          │
│                                    │                   │
│                                    ▼                   │
│  ┌─────────────────────────────────────────────────┐   │
│  │         Stage 2: Token-Level Detection           │   │
│  │         + NLI Explanation (~64-150ms)            │   │
│  │                                                  │   │
│  │    "어떤 토큰이 틀렸고, 왜 틀렸는가?"            │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 📋 Stage 1: HaluGate Sentinel (프롬프트 분류)

모든 쿼리에 hallucination 탐지를 실행할 필요는 없습니다.

### 분류 기준

| 프롬프트 | Fact-Check 필요? | 이유 |
|---------|-----------------|------|
| "아인슈타인은 언제 태어났나요?" | ✅ Yes | 검증 가능한 사실 |
| "가을에 대한 시를 써줘" | ❌ No | 창작 작업 |
| "이 파이썬 코드 디버깅해줘" | ❌ No | 기술 지원 |
| "AI에 대한 의견은?" | ❌ No | 의견 요청 |
| "지구는 둥근가요?" | ✅ Yes | 사실적 주장 |

### 왜 사전 분류가 중요한가?

```
┌─────────────────────────────────────────────────────────┐
│                 효율성 계산                              │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Token-Level Detection 비용:                            │
│  - 4K 토큰 컨텍스트: ~125ms                             │
│  - 16K 토큰 컨텍스트: ~365ms                            │
│                                                         │
│  프로덕션 워크로드에서:                                  │
│  - ~35%의 쿼리가 비사실적 (창작, 코딩, 의견)             │
│                                                         │
│  사전 분류 적용 시:                                      │
│  → 72.2% 효율성 향상! (불필요한 탐지 스킵)              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Sentinel 모델 구조

```
┌─────────────────────────────────────────────────────────┐
│                HaluGate Sentinel 모델                    │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Base: ModernBERT-base + LoRA adapter                   │
│                                                         │
│  학습 데이터 (50,000 샘플, 14개 데이터셋):               │
│                                                         │
│  ✅ Fact-Check 필요 (Positive):                         │
│     - SQuAD, TriviaQA, Natural Questions, HotpotQA     │
│     - TruthfulQA (일반적 오해들)                        │
│     - HaluEval, FactCHD                                │
│     - FaithDial, CoQA                                  │
│                                                         │
│  ❌ Fact-Check 불필요 (Negative):                       │
│     - WritingPrompts (창작)                            │
│     - CodeSearchNet (코드)                             │
│     - Dolly non-factual, Alpaca creative              │
│                                                         │
│  성능: 96.4% 검증 정확도, ~12ms 추론 지연               │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 🔬 Stage 2: Token-Level Detection + NLI Explanation

사실 확인이 필요한 프롬프트에 대해 2개 모델을 순차 실행합니다.

### Token-Level Hallucination Detection

기존 문장 레벨 분류기가 "hallucinated/not hallucinated" 단일 라벨을 출력하는 것과 달리, **토큰 레벨 탐지**는 정확히 어떤 토큰이 문제인지 식별합니다.

```
┌─────────────────────────────────────────────────────────┐
│              Token-Level Detection 구조                  │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  입력:                                                  │
│  [CLS] context [SEP] question [SEP] answer [SEP]       │
│                                                         │
│           │                                             │
│           ▼                                             │
│  ┌─────────────────────────────────────────────────┐   │
│  │            ModernBERT Encoder                    │   │
│  └─────────────────────┬───────────────────────────┘   │
│                        │                                │
│                        ▼                                │
│  ┌─────────────────────────────────────────────────┐   │
│  │     Token Classification Head (토큰별 이진분류)  │   │
│  └─────────────────────┬───────────────────────────┘   │
│                        │                                │
│                        ▼                                │
│                                                         │
│  출력 (answer 토큰만):                                  │
│  "에펠탑은  1950년에  지어졌으며  500미터  입니다"       │
│      0       1        0         1        0             │
│            ^^^^                ^^^^                     │
│         Hallucinated!       Hallucinated!              │
│                                                         │
│  0 = Supported (지원됨)                                 │
│  1 = Hallucinated (환각)                               │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### NLI Explanation Layer

어떤 것이 hallucination인지 아는 것만으로는 충분하지 않습니다. **왜** 틀렸는지 알아야 합니다.

```
┌─────────────────────────────────────────────────────────┐
│                NLI (자연어 추론) 분류                     │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  입력:                                                  │
│  ┌─────────────────────────────────────────────────┐   │
│  │ Premise (Context):                               │   │
│  │ "에펠탑은 1887-1889년에 건설되었다"              │   │
│  └─────────────────────────────────────────────────┘   │
│                        +                                │
│  ┌─────────────────────────────────────────────────┐   │
│  │ Hypothesis (Detected Span):                      │   │
│  │ "1950년에 건설"                                  │   │
│  └─────────────────────────────────────────────────┘   │
│                        │                                │
│                        ▼                                │
│                                                         │
│  출력:                                                  │
│  ┌─────────────────────────────────────────────────┐   │
│  │           🔴 CONTRADICTION                       │   │
│  │     "1950"은 "1887-1889"와 직접 모순됨           │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### NLI 라벨의 의미

| NLI 라벨 | 의미 | 심각도 | 조치 |
|---------|------|-------|------|
| **CONTRADICTION** | 주장이 컨텍스트와 충돌 | 4 (높음) | 오류로 플래그 |
| **NEUTRAL** | 컨텍스트로 검증 불가 | 2 (중간) | 검증 불가로 플래그 |
| **ENTAILMENT** | 컨텍스트가 주장을 지지 | 0 | False Positive 필터링 |

### 왜 앙상블이 효과적인가?

```
┌─────────────────────────────────────────────────────────┐
│             단일 모델 vs 앙상블 접근법                    │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Token-Level Detection 단독:                            │
│  - Hallucinated 클래스 F1: 59%                         │
│  - 절반 가까이 놓침 + 1/3이 False Positive              │
│                                                         │
│  통합 5-class 모델 시도:                                │
│  (SUPPORTED/CONTRADICTION/FABRICATION/...)              │
│  - F1: 21.7% (처참한 성능)                             │
│  - 토큰 레벨로는 "왜" 틀렸는지 구분 불가                 │
│                                                         │
│  2단계 앙상블 (HaluGate):                               │
│  ┌─────────────────────────────────────────────────┐   │
│  │  Token Detection  →  Recall 확보 (잠재 이슈 포착)│   │
│  │        +                                        │   │
│  │  NLI Explainer   →  Precision + Explainability │   │
│  │                     (FP 필터링 + 이유 설명)      │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
│  → 보통 수준의 탐지기를 실용적 시스템으로 전환!          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 📡 Response Headers: 투명한 결과 전달

탐지 결과는 HTTP 헤더로 전달되어 다운스트림 시스템이 커스텀 정책을 구현할 수 있습니다.

### Hallucination 탐지 시

```http
HTTP/1.1 200 OK
Content-Type: application/json
x-vsr-fact-check-needed: true
x-vsr-hallucination-detected: true
x-vsr-hallucination-spans: 1950; 500 meters
x-vsr-nli-contradictions: 2
x-vsr-max-severity: 4
```

### 검증 불가 응답 시 (Tool 없음)

```http
HTTP/1.1 200 OK
x-vsr-fact-check-needed: true
x-vsr-unverified-factual-response: true
x-vsr-verification-context-missing: true
```

### 헤더 활용 시나리오

| 활용 | 설명 |
|------|------|
| **UI 경고 표시** | 신뢰도 낮을 때 사용자에게 경고 |
| **휴먼 리뷰 큐** | 플래그된 응답을 수동 검토로 라우팅 |
| **감사 로깅** | 미검증 주장을 컴플라이언스용 추적 |
| **조건부 차단** | 심각도 높은 CONTRADICTION 차단 |

---

## 🛤️ 전체 파이프라인: 3가지 경로

```
┌─────────────────────────────────────────────────────────┐
│                HaluGate 처리 경로                        │
├─────────────────────────────────────────────────────────┤
│                                                         │
│                    사용자 요청                           │
│                        │                                │
│                        ▼                                │
│             ┌──────────────────┐                       │
│             │  Sentinel 분류   │                       │
│             └────────┬─────────┘                       │
│                      │                                  │
│        ┌─────────────┼─────────────┐                   │
│        │             │             │                   │
│        ▼             ▼             ▼                   │
│   ┌─────────┐  ┌───────────┐  ┌───────────┐          │
│   │ Path 1  │  │  Path 2   │  │  Path 3   │          │
│   │비사실적 │  │사실적+    │  │사실적+    │          │
│   │ 프롬프트│  │Tool 없음  │  │Tool 있음  │          │
│   └────┬────┘  └─────┬─────┘  └─────┬─────┘          │
│        │             │              │                  │
│        ▼             ▼              ▼                  │
│   Pass Through   경고 헤더     전체 탐지               │
│   (~12ms)       추가 (~12ms)  + 헤더                  │
│                              (76-162ms)               │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

| 경로 | 조건 | 추가 지연 | 조치 |
|------|------|----------|------|
| **Path 1** | 비사실적 프롬프트 | ~12ms (분류만) | 그대로 통과 |
| **Path 2** | 사실적 + Tool 없음 | ~12ms | 경고 헤더 추가 |
| **Path 3** | 사실적 + Tool 있음 | 76-162ms | 전체 탐지 + 헤더 |

---

## ⚡ 왜 Native Rust/Candle인가?

HaluGate의 세 모델은 모두 **Candle** (Hugging Face의 Rust ML 프레임워크)로 네이티브 실행됩니다.

```
┌─────────────────────────────────────────────────────────┐
│            Python vs Native 비교                         │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  측면          │ Python (PyTorch) │ Native (Candle)     │
│  ─────────────│──────────────────│─────────────────     │
│  Cold Start   │ 5-10초           │ <500ms              │
│  메모리       │ 모델당 2-4GB     │ 모델당 500MB-1GB    │
│  지연시간     │ +50-100ms 오버헤드│ 거의 제로 오버헤드  │
│  배포         │ Python 런타임 필요│ 단일 바이너리       │
│  스케일링     │ GIL 경합         │ 진정한 병렬성       │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

별도의 Python 서비스, 사이드카, 모델 서버 없이 **모든 것이 인프로세스로 실행**됩니다.

### 지연시간 상세

| 컴포넌트 | P50 | P99 | 비고 |
|---------|-----|-----|------|
| Fact-check 분류기 | 12ms | 28ms | ModernBERT 추론 |
| Tool 컨텍스트 추출 | 1ms | 3ms | JSON 파싱 |
| Hallucination 탐지기 | 45ms | 89ms | 토큰 분류 |
| NLI 설명기 | 18ms | 42ms | 스팬별 분류 |
| **총 오버헤드** | **76ms** | **162ms** | 탐지 실행 시 |

일반적인 LLM 생성 시간(5-30초)과 비교하면 무시할 수 있는 수준입니다.

---

## 📊 설정 예시

```yaml
# 모델 설정
hallucination_mitigation:
  # Stage 1: 프롬프트 분류
  fact_check_model:
    model_id: "models/halugate-sentinel"
    threshold: 0.6  # FACT_CHECK_NEEDED 신뢰도 임계값
    use_cpu: true

  # Stage 2a: 토큰 레벨 탐지
  hallucination_model:
    model_id: "models/halugate-detector"
    threshold: 0.8  # 토큰 신뢰도 임계값
    use_cpu: true

  # Stage 2b: NLI 설명
  nli_model:
    model_id: "models/halugate-explainer"
    threshold: 0.9  # NLI 신뢰도 임계값
    use_cpu: true

# Hallucination 플러그인이 포함된 Decision
decisions:
  - name: "verified-factual"
    priority: 100
    rules:
      operator: "AND"
      conditions:
        - type: "fact_check"
          name: "needs_fact_check"
    plugins:
      - type: "hallucination"
        configuration:
          enabled: true
          use_nli: true
          hallucination_action: "header"  # header | body | block | none
          unverified_factual_action: "header"
          include_hallucination_details: true
```

### Action 옵션

| Action | 동작 |
|--------|------|
| `header` | 경고 헤더 추가, 응답은 통과 |
| `body` | 응답 본문에 경고 주입 |
| `block` | 오류 응답 반환, LLM 출력 전달 안 함 |
| `none` | 로그만, 사용자에게 표시 안 함 |

---

## 📈 평가 프레임워크로서의 HaluGate

HaluGate는 실시간 프로덕션 사용뿐 아니라 **오프라인 모델 평가**에도 활용할 수 있습니다.

```
┌─────────────────────────────────────────────────────────┐
│              평가 워크플로우                              │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. 데이터셋 로드                                        │
│     - TriviaQA, Natural Questions, HotpotQA            │
│     - 또는 기업 커스텀 데이터셋                          │
│                                                         │
│  2. 응답 생성                                            │
│     - 테스트 대상 모델로 각 쿼리 실행                    │
│                                                         │
│  3. Hallucination 탐지                                  │
│     - (context, query, response) → HaluGate Detector    │
│                                                         │
│  4. 심각도 분류                                          │
│     - HaluGate Explainer로 각 스팬 카테고리화           │
│                                                         │
│  5. 메트릭 집계                                          │
│     - Hallucination 비율, Contradiction 비율 등         │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## ⚠️ 한계점

HaluGate는 **Extrinsic Hallucination**에 특화되어 있습니다. 알려진 한계가 있습니다.

### 탐지 불가능한 케이스

| 한계 | 예시 | 이유 |
|------|------|------|
| **Intrinsic Hallucination** | Tool 호출 없이 "아인슈타인은 1900년생" | 검증할 컨텍스트 없음 |
| **No-context 시나리오** | 사실적 질문인데 Tool이 정의 안 됨 | Ground Truth 없음 |

### 투명한 Degradation

Tool 컨텍스트가 없는 사실 확인 요청에 대해서는 명시적으로 "unverified factual"로 플래그합니다:

```http
x-vsr-fact-check-needed: true
x-vsr-unverified-factual-response: true
x-vsr-verification-context-missing: true
```

조용히 통과시키는 대신 불확실성을 명시적으로 전달합니다.

---

## 🎓 핵심 인사이트 정리

### 1. Function Calling = 이미 존재하는 Ground Truth

```
"별도 인프라 구축 없이, 기존 Tool 응답을 Ground Truth로 활용"

RAG를 위한 별도 retrieval 없이
Function Calling API 흐름 자체가 검증 기반을 제공합니다.
```

### 2. 조건부 실행으로 효율성 확보

```
"모든 쿼리를 검증할 필요 없다"

창작, 코딩, 의견 요청 = 스킵
사실 확인이 필요한 쿼리만 집중 탐지

→ 72.2% 효율성 향상
```

### 3. 토큰 레벨 + NLI 앙상블

```
"무엇이 틀렸는지 + 왜 틀렸는지"

Token Detection: Recall 확보
NLI Explanation: Precision + 설명가능성

→ 단일 모델의 한계를 앙상블로 극복
```

### 4. Native 실행의 중요성

```
"Python 없이, 사이드카 없이, 인프로세스로"

76-162ms 오버헤드 = LLM 생성 시간 대비 무시 가능
CPU만으로 실행 = GPU 리소스 절약
```

---

## 🔮 시사점

### 프로덕션 LLM 배포에 주는 교훈

1. **Hallucination은 피할 수 없다**: GPT-5.2조차 버전별 hallucination 차이가 있음
2. **사후 검증이 필수**: 아무리 좋은 모델이라도 Ground Truth 대비 검증 필요
3. **효율적 검증 가능**: LLM Judge 없이 밀리초 단위 검증 달성 가능
4. **투명성이 핵심**: 헤더로 결과 전달 → 다운스트림에서 정책 결정

### RAG 시스템에 적용할 점

| 요소 | HaluGate 접근법 | 적용 가능성 |
|------|----------------|------------|
| 사전 분류 | Sentinel로 비사실적 쿼리 스킵 | 모든 RAG 시스템 |
| 토큰 레벨 탐지 | 정확히 어떤 부분이 틀렸는지 | 고신뢰 요구 도메인 |
| NLI 설명 | 왜 틀렸는지 분류 | 감사/컴플라이언스 |
| 헤더 기반 전달 | 다운스트림 정책 결정 분리 | 마이크로서비스 아키텍처 |

---

## 📚 참고 자료

- **원문 블로그**: [Token-Level Truth: Real-Time Hallucination Detection](https://blog.vllm.ai/2025/12/14/halugate.html)
- **Signal-Decision Architecture**: [vLLM Blog](https://blog.vllm.ai/2025/11/19/signal-decision.html)
- **HaluGate Sentinel 모델**: [Hugging Face](https://huggingface.co/llm-semantic-router/halugate-sentinel)
- **vLLM Semantic Router GitHub**: [GitHub Repo](https://github.com/vllm-project/semantic-router)
- **LettuceDetect (영감을 준 연구)**: [KRLabs GitHub](https://github.com/KRLabsOrg/LettuceDetect)

---

*이 글이 도움이 되셨다면 공유해 주세요! 🙏*
